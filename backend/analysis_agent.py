import pandas as pd
import os
import uuid
import re
from dotenv import load_dotenv
from typing import Dict, Optional, List, Literal
from pydantic import SecretStr
from langchain_openai import ChatOpenAI
from langchain_experimental.agents import create_pandas_dataframe_agent
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    trim_messages,
)
from langchain_core.messages.utils import count_tokens_approximately
load_dotenv()

# Define the state
class AgentState(Dict):
    route: Literal["analysis", "chat"]
    history_messages: Optional[List[Dict]] = None # store all messages
    file_paths: Optional[List[str]] = None # store all file paths
    user_prompt: Optional[str] = None # user prompt
    raw_output: Optional[str] = None#LLM output
    exec_code: Optional[str] = None#code after analysis
    analysis_dataframe_dict: Optional[Dict] = None#dataframe after analysis
    filtered_data_summary: Optional[str] = None#summary of filtered data
    error: Optional[str] = None#error message

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY environment variable is not set")
llm = ChatOpenAI(model="gpt-4o-mini", api_key=SecretStr(api_key))

#clean up node
def clean_up_node(state: AgentState) -> AgentState:
    state["raw_output"] = None
    state["exec_code"] = None
    state["analysis_dataframe_dict"] = None
    state["filtered_data_summary"] = None
    state["error"] = None
    # keep history_messages, file_paths, user_prompt
    return state

# ä¿®æ”¹input_nodeä»¥æ¥å—å¤–éƒ¨ä¼ å…¥çš„æ–‡ä»¶è·¯å¾„å’Œprompt
# æ³¨æ„ï¼šæ­¤å‡½æ•°å·²ä¸å†ä½¿ç”¨ï¼Œé€»è¾‘å·²ç§»è‡³run_analysiså‡½æ•°ä¸­

#router node
def router_node(state: AgentState) -> AgentState:
    
    prompt = f"""You are a data analysis assistant.
    You will receive a user's question.
    
    Context:
    Determine whether the user's message is:
    - "analysis" (if the user is asking to analyze or query data)
    - "chat" (if it's casual conversation, general questions)

    User message: "{state["user_prompt"]}"

    Respond with either "analysis" or "chat".
    """
    
    result = llm.invoke([HumanMessage(content=prompt)]).content.strip().lower()
    if "analysis" in result:
        state["route"] = "analysis"
    else:
        state["route"] = "chat"
    return state

def extract_code_blocks(text:str):
    code_blocks = re.findall(r"```(?:python)?\n(.*?)```", text, re.DOTALL)
    return code_blocks[0] if code_blocks else ""

def analysis_node(state: AgentState) -> AgentState:
    if state.get("error"):
        return state
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶è·¯å¾„
    if not state.get("file_paths") or len(state["file_paths"]) == 0:
        state["error"] = "No files provided for analysis. Please upload files first or use chat mode for general questions."
        return state
    
    dfs = []
    for file_path in state["file_paths"]:
            if file_path.endswith(".csv"):
                df = pd.read_csv(file_path)
            elif file_path.endswith(".xlsx"):
                df = pd.read_excel(file_path)
            else:
                raise ValueError(f"Unsupported file type: {file_path}")
            dfs.append(df)
  
    agent = create_pandas_dataframe_agent(
        llm, dfs, verbose=True, allow_dangerous_code=True,
        agent_type="openai-tools", return_intermediate_steps=True,
    )
    
    try:
        invoke_result = agent.invoke(state["history_messages"])
        raw_output = invoke_result["output"]
        state["raw_output"] = raw_output

        # trim history messages
        state["history_messages"].append(AIMessage(raw_output))#raw_output is code here
        state["history_messages"] = trim_messages(
            state["history_messages"],
            strategy="last",
            max_tokens=30000,
            token_counter=count_tokens_approximately
            ) 
        
        state["exec_code"] = extract_code_blocks(raw_output)
    except Exception as e:
        state["error"] = f"analysis failed: {str(e)}"
    return state

#execute code
def execute_code_node(state: AgentState) -> AgentState:
    if state.get("error"):
        return state
    if not state.get("exec_code"):
        return state
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶è·¯å¾„
    if not state.get("file_paths") or len(state["file_paths"]) == 0:
        state["error"] = "No files available for code execution."
        return state
    
    dfs = []
    for file_path in state["file_paths"]:
            if file_path.endswith(".csv"):
                df = pd.read_csv(file_path)
            elif file_path.endswith(".xlsx"):
                df = pd.read_excel(file_path)
            dfs.append(df)
            
    exec_env = {"dfs": dfs, "pd": pd}

    try:
        exec(state["exec_code"], exec_env)

        if "result" not in exec_env:
            state["error"] = "âŒ No variable named `result` was defined in the executed code."
            return state

        result = exec_env["result"]

        # æ¸…ç†æ•°æ®çš„è¾…åŠ©å‡½æ•°
        def clean_dataframe_for_json(df):
            """æ¸…ç†DataFrameä¸­çš„æ— ç©·å¤§å€¼å’ŒNaNå€¼ï¼Œä½¿å…¶èƒ½å¤Ÿæ­£ç¡®åºåˆ—åŒ–ä¸ºJSON"""
            try:
                df_cleaned = df.copy()
                # åªå¯¹æ•°å€¼åˆ—è¿›è¡Œæ¸…ç†
                numeric_columns = df_cleaned.select_dtypes(include=[float, int]).columns
                for col in numeric_columns:
                    # æ›¿æ¢æ— ç©·å¤§å€¼
                    df_cleaned[col] = df_cleaned[col].replace([float('inf'), float('-inf')], None)
                    # æ›¿æ¢NaNå€¼
                    df_cleaned[col] = df_cleaned[col].where(pd.notna(df_cleaned[col]), None)
                return df_cleaned
            except Exception as e:
                # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œè¿”å›åŸå§‹DataFrameå¹¶ç”¨fillnaå¤„ç†
                return df.fillna("N/A")
        
        # deal with different types of result
        if isinstance(result, pd.DataFrame):
            cleaned_df = clean_dataframe_for_json(result)
            state["analysis_dataframe_dict"] = cleaned_df.to_dict(orient="records")
        elif isinstance(result, pd.Series):
            df = result.reset_index()
            cleaned_df = clean_dataframe_for_json(df)
            state["analysis_dataframe_dict"] = cleaned_df.to_dict(orient="records")
        elif isinstance(result, dict):
            # å¤„ç†å­—å…¸ç±»å‹çš„ç»“æœ
            try:
                # æ£€æŸ¥å­—å…¸ä¸­æ˜¯å¦åŒ…å«DataFrameæˆ–Serieså¯¹è±¡
                flattened_data = []
                for key, value in result.items():
                    if isinstance(value, pd.DataFrame):
                        # å¦‚æœå€¼æ˜¯DataFrameï¼Œå°†å…¶è½¬æ¢ä¸ºè®°å½•æ ¼å¼å¹¶æ ‡è®°ç±»åˆ«
                        df_records = value.to_dict(orient="records")
                        for i, record in enumerate(df_records):
                            for col, cell_value in record.items():
                                flattened_data.append({
                                    "category": str(key),
                                    "metric": str(col),
                                    "value": cell_value
                                })
                    elif isinstance(value, pd.Series):
                        # å¦‚æœå€¼æ˜¯Seriesï¼Œå±•å¹³å®ƒ
                        for idx, val in value.items():
                            flattened_data.append({
                                "category": str(key),
                                "metric": str(idx),
                                "value": val
                            })
                    else:
                        # ç®€å•å€¼
                        flattened_data.append({
                            "category": str(key),
                            "metric": "value",
                            "value": value
                        })
                
                if flattened_data:
                    df = pd.DataFrame(flattened_data)
                    cleaned_df = clean_dataframe_for_json(df)
                    state["analysis_dataframe_dict"] = cleaned_df.to_dict(orient="records")
                else:
                    # å¦‚æœå±•å¹³å¤±è´¥ï¼Œå°è¯•ç›´æ¥è½¬æ¢
                    df = pd.DataFrame([{str(k): str(v) for k, v in result.items()}])
                    cleaned_df = clean_dataframe_for_json(df)
                    state["analysis_dataframe_dict"] = cleaned_df.to_dict(orient="records")
            except Exception as e:
                # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œåˆ›å»ºé”®å€¼å¯¹çš„è¡¨æ ¼
                df = pd.DataFrame([{"Key": str(k), "Value": str(v)} for k, v in result.items()])
                state["analysis_dataframe_dict"] = df.to_dict(orient="records")
        elif isinstance(result, (tuple, list)):
            try:
                # å°è¯•åˆ›å»ºDataFrame
                if all(not isinstance(i, (list, tuple, dict)) for i in result):
                    # ç®€å•å€¼åˆ—è¡¨
                    df = pd.DataFrame(result, columns=["value"])
                else:
                    # å¤æ‚ç»“æ„
                    df = pd.DataFrame(result)
                cleaned_df = clean_dataframe_for_json(df)
                state["analysis_dataframe_dict"] = cleaned_df.to_dict(orient="records")
            except Exception as e:
                # å¦‚æœåˆ›å»ºDataFrameå¤±è´¥ï¼Œè½¬æ¢ä¸ºç®€å•çš„é”®å€¼å¯¹
                df = pd.DataFrame([{"index": i, "value": str(v)} for i, v in enumerate(result)])
                state["analysis_dataframe_dict"] = df.to_dict(orient="records")
        elif isinstance(result, (int, float, str)):
            # å¤„ç†å•ä¸ªå€¼çš„æƒ…å†µï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæ— ç©·å¤§æˆ–NaN
            if isinstance(result, float) and (pd.isna(result) or pd.isinf(result)):
                result = None
            df = pd.DataFrame([{"value": result}])
            state["analysis_dataframe_dict"] = df.to_dict(orient="records")
        elif result is None:
            # å¤„ç†Noneç»“æœï¼Œå¯èƒ½æ˜¯å¯è§†åŒ–ä»£ç æ²¡æœ‰è¿”å›æ•°æ®
            state["analysis_dataframe_dict"] = [{"message": "åˆ†æå®Œæˆï¼Œç»“æœå·²é€šè¿‡å›¾è¡¨æ˜¾ç¤º"}]
        else:
            state["error"] = f"âŒ Unsupported result type: {type(result)}"
            return state
    except Exception as e:
        state["error"] = f"âŒ Code execution error: {str(e)}"
    return state

#analyze filtered data
def analysis_filtered_data_node(state: AgentState) -> AgentState:
    if state.get("error"):
        return state
    
    system_prompt = f"""You are a data analysis assistant. You will receive:
    1. A user's question.
    2. A table that is the result of a Python data analysis.
    Your task is to interpret the table and write a clear, concise natural language summary that answers the user's question.

    Guidelines:
    - Focus on key insights from the table (e.g., trends, top performers, comparisons).
    - Keep your summary to 2-3 sentences.
    - If the table is empty or inconclusive, explain that no results were found.

    Now summarize the following result.
    """
    
    result = llm.invoke([SystemMessage(system_prompt), 
                         HumanMessage(state["user_prompt"]+ f"\n\nanalysis_dataframe: {pd.DataFrame(state['analysis_dataframe_dict'])}")
        ]).content
    state["filtered_data_summary"] = result
    state["history_messages"][-1].content += "\n\n" + result
    return state

#chat node
def chat_node(state: AgentState) -> AgentState:
    if state.get("error"):
        return state
    if not state.get("history_messages"):
        state["error"] = "No history messages provided for chat."
        return state
    result = llm.invoke(state["history_messages"]).content
    state["filtered_data_summary"] = result
    state["raw_output"] = result
    state["history_messages"].append(AIMessage(result))
    return state

#output node
def output_node(state: AgentState) -> AgentState:
    print("="*100)
    if state.get("analysis_dataframe_dict"):
        print(pd.DataFrame(state['analysis_dataframe_dict']))
    print(state['filtered_data_summary'])
    print("="*100)
    return state

# åˆ›å»ºä¸“é—¨ç”¨äºAPIçš„å›¾æ„å»ºå™¨ï¼Œä¸åŒ…å«inputèŠ‚ç‚¹
#create graph
builder = StateGraph(AgentState)
builder.add_node("clean_up", clean_up_node)
builder.add_node("router", router_node)
builder.add_node("analysis", analysis_node)
builder.add_node("execute_code", execute_code_node)
builder.add_node("analysis_filtered_data", analysis_filtered_data_node)
builder.add_node("chat", chat_node)
builder.add_node("output", output_node)


builder.add_edge(START, "clean_up")
builder.add_edge("clean_up", "router")
builder.add_conditional_edges(
    source="router",
    path=lambda state: state["route"],
    path_map={
        "analysis": "analysis",
        "chat": "chat"
    }
)
builder.add_edge("chat", "output")
builder.add_edge("analysis", "execute_code")
builder.add_edge("execute_code", "analysis_filtered_data")
builder.add_edge("analysis_filtered_data", "output")
builder.add_edge("output", END)

memory = MemorySaver()
graph =builder.compile(checkpointer=memory)


# APIè°ƒç”¨çš„ä¸»å‡½æ•°
def run_analysis(file_paths: List[str], prompt: str, session_id: str = None) -> Dict:
    """
    è¿è¡Œæ•°æ®åˆ†æ
    
    Args:
        file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        prompt: ç”¨æˆ·åˆ†ææŒ‡ä»¤
        session_id: ä¼šè¯IDï¼Œç”¨äºä¿æŒå¯¹è¯å†å²è¿ç»­æ€§
        
    Returns:
        åŒ…å«åˆ†æç»“æœçš„å­—å…¸
    """
    try:
        # ä½¿ç”¨æä¾›çš„session_idæˆ–ç”Ÿæˆæ–°çš„
        if session_id:
            config = {"configurable": {"thread_id": session_id}}
        else:
            session_id = str(uuid.uuid4())
            config = {"configurable": {"thread_id": session_id}}
        
       
        
        
        # å°è¯•è·å–ç°æœ‰çŠ¶æ€ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°çŠ¶æ€
        try:
            # è·å–ç°æœ‰çš„çŠ¶æ€å¿«ç…§
            snapshot = graph.get_state(config)
            if snapshot and snapshot.values:
                # ä½¿ç”¨ç°æœ‰çŠ¶æ€
                state = snapshot.values
                print(f"ğŸ“š æ¢å¤ä¼šè¯è®°å¿†ï¼Œå†å²æ¶ˆæ¯æ•°é‡: {len(state.get('history_messages', []))}")
            else:
                # åˆ›å»ºæ–°çŠ¶æ€
                state = AgentState()
                print(f"ğŸ†• åˆ›å»ºæ–°ä¼šè¯: {session_id}")
        except Exception as e:
            # å¦‚æœè·å–çŠ¶æ€å¤±è´¥ï¼Œåˆ›å»ºæ–°çŠ¶æ€
            state = AgentState()
            print(f"âš ï¸ æ— æ³•æ¢å¤çŠ¶æ€ï¼Œåˆ›å»ºæ–°ä¼šè¯: {e}")
        
        # è®¾ç½®å½“å‰è¯·æ±‚çš„å‚æ•°
        state["file_paths"] = file_paths
        state["user_prompt"] = prompt
        
        # å¦‚æœæ˜¯æ–°çŠ¶æ€æˆ–æ²¡æœ‰å†å²æ¶ˆæ¯ï¼Œåˆå§‹åŒ–ç³»ç»Ÿæ¶ˆæ¯
        if not state.get("history_messages"):
            # ä½¿ç”¨é€šç”¨çš„ç³»ç»Ÿæç¤ºï¼Œæ—¢æ”¯æŒæ•°æ®åˆ†æåˆæ”¯æŒæ™®é€šèŠå¤©
            system_prompt = """You are a helpful AI assistant with expertise in data analysis. You can:

            1. **Data Analysis**: When users upload CSV or Excel files, you can analyze the data using Python and pandas. Generate Python code to extract insights from datasets.
            2. **General Conversation**: Have friendly conversations on any topic and remember our previous discussion context.

            For data analysis tasks:
            - Assume DataFrames are available as dfs = [dfs[0], dfs[1], dfs[2] ...] 
            - IMPORTANT: DataFrames are ONLY available as a list: dfs = [dfs[0], dfs[1], dfs[2], ...]
            - ALWAYS use dfs[0] for the first DataFrame, dfs[1] for the second, etc.
            - NEVER use variable names like 'df', 'data', 'dataset' - these are NOT defined
            - ONLY use dfs[0], dfs[1], dfs[2], etc. to reference DataFrames
            - The last line MUST assign a meaningful result to a variable named `result`
            - `result` should contain actual data: DataFrame, Series, list, dict, or single value
            - NEVER set result = None
            - Always return actual data that answers the user's question
            - Do NOT execute code, just generate it

            For general conversation:
            - Be friendly, helpful, and maintain context from our conversation history
            - Remember user preferences and previous topics we've discussed

            CORRECT Examples for data analysis:
            ```python
            # Show top 3 highest salaries from first DataFrame
            result = dfs[0].nlargest(3, 'Salary')
            ```

            ```python
            # Calculate summary statistics from second DataFrame
            result = dfs[1]['Salary'].describe()
            ```

            ```python
            # Group and aggregate data from third DataFrame
            result = dfs[2].groupby('Category')['Value'].sum()
            ```

            ```python
            # Work with multiple DataFrames
            merged_data = pd.concat([dfs[0], dfs[1]], ignore_index=True)
            result = merged_data.groupby('Type').count()
            ```

            WRONG Examples (DO NOT USE):
            ```python
            # âŒ WRONG - 'df' is not defined
            result = df.head()
            
            # âŒ WRONG - 'data' is not defined  
            result = data.describe()
            ```

            Remember: ONLY use dfs[0], dfs[1], etc. - no other DataFrame variable names exist!"""
            
            state["history_messages"] = [SystemMessage(system_prompt)]
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        state["history_messages"].append(HumanMessage(prompt))
        
        # æ‰§è¡Œåˆ†æ
        result_state = graph.invoke(state, config=config)
        
        # å‡†å¤‡è¿”å›ç»“æœ
        response = {
            "status": "success",
            "summary": result_state.get("filtered_data_summary", ""),
            "data": result_state.get("analysis_dataframe_dict", []),
            "code": result_state.get("exec_code", ""),
            "error": result_state.get("error"),
            "session_id": session_id
        }
        
        return response
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "summary": "",
            "data": [],
            "code": "",
            "session_id": session_id if session_id else ""
        }